{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "import os\n",
    "from argparse import ArgumentParser\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import wandb\n",
    "import torch\n",
    "from denoising_diffusion_pytorch import Unet, GaussianDiffusion, Trainer\n",
    "from encoder_decoders.vq_vae_encdec import VQVAEEncoder, VQVAEDecoder\n",
    "from vector_quantization import VectorQuantize\n",
    "from utils import load_yaml_param_settings, get_root_dir, freeze\n",
    "from einops import rearrange\n",
    "\n",
    "from stage2 import load_pretrained_encoder_decoder_vq"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "def load_args():\n",
    "    parser = ArgumentParser()\n",
    "    parser.add_argument('--config', type=str, help=\"Path to the config data  file.\",\n",
    "                        default=get_root_dir().joinpath('configs', 'config.yaml'))\n",
    "    return parser.parse_args([])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[34m\u001B[1mwandb\u001B[0m: Currently logged in as: \u001B[33mdaesoolee\u001B[0m. Use \u001B[1m`wandb login --relogin`\u001B[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "wandb version 0.13.11 is available!  To upgrade, please run:\n $ pip install wandb --upgrade"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Tracking run with wandb version 0.13.10"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Run data is saved locally in <code>C:\\projects\\denoising-diffusion-pytorch\\wandb\\run-20230313_134255-dobvf7h2</code>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Syncing run <strong><a href='https://wandb.ai/daesoolee/GeoDiffusion-stage2/runs/dobvf7h2' target=\"_blank\">giddy-wildflower-76</a></strong> to <a href='https://wandb.ai/daesoolee/GeoDiffusion-stage2' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": " View project at <a href='https://wandb.ai/daesoolee/GeoDiffusion-stage2' target=\"_blank\">https://wandb.ai/daesoolee/GeoDiffusion-stage2</a>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": " View run at <a href='https://wandb.ai/daesoolee/GeoDiffusion-stage2/runs/dobvf7h2' target=\"_blank\">https://wandb.ai/daesoolee/GeoDiffusion-stage2/runs/dobvf7h2</a>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data loading... 0%\n",
      "Data loading... 50%\n",
      "self.X_train.shape: (140, 4, 128, 128)\n",
      "self.X_test.shape: (60, 4, 128, 128)\n"
     ]
    }
   ],
   "source": [
    "# Load the trained LDM\n",
    "\n",
    "# load config\n",
    "args = load_args()\n",
    "config = load_yaml_param_settings(args.config)\n",
    "\n",
    "# load the pretrained encoder, decoder, and vq\n",
    "encoder, decoder, vq_model = load_pretrained_encoder_decoder_vq(config, 'saved_models', freeze_models=True)\n",
    "encoder, decoder, vq_model = encoder.cuda(), decoder.cuda(), vq_model.cuda()\n",
    "\n",
    "# model\n",
    "model = Unet(\n",
    "        in_channels=config['VQ-VAE']['codebook_dim'],\n",
    "        dim=64,\n",
    "        dim_mults=(1, 2, 4, 8),\n",
    "        self_condition=config['diffusion']['unet']['self_condition'],\n",
    ").cuda()\n",
    "\n",
    "diffusion = GaussianDiffusion(\n",
    "    model,\n",
    "    in_size=encoder.H_prime[0].item(),  # width or height of z\n",
    "    timesteps=1000,  # number of steps\n",
    "    sampling_timesteps=1000,\n",
    "    # number of sampling timesteps (using ddim for faster inference [see citation for ddim paper])\n",
    "    loss_type='l1',  # L1 or L2\n",
    "    auto_normalize=False,\n",
    ").cuda()\n",
    "\n",
    "# train\n",
    "trainer = Trainer(\n",
    "    diffusion,\n",
    "    config,\n",
    "    encoder,\n",
    "    decoder,\n",
    "    vq_model,\n",
    "    train_batch_size=config['dataset']['batch_sizes']['stage2'],\n",
    "    train_lr=8e-5,\n",
    "    train_num_steps=700000,  # total training steps\n",
    "    gradient_accumulate_every=2,  # gradient accumulation steps\n",
    "    ema_decay=0.995,  # exponential moving average decay\n",
    "    amp=False,  # turn on mixed precision\n",
    "    fp16=False,\n",
    "    save_and_sample_every=1000, #1000,\n",
    "    num_samples=9,\n",
    "    augment_horizontal_flip=False\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading from version 1.2.2\n"
     ]
    }
   ],
   "source": [
    "# load the pretrained LDM\n",
    "trainer.load(milestone=130)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "# sample z\n",
    "trainer.ema.ema_model.eval()\n",
    "z_gen = trainer.ema.ema_model.sample(batch_size=9)\n",
    "\n",
    "print('z_gen.shape:', z_gen.shape)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "data": {
      "text/plain": "sampling loop time step:   0%|          | 0/1000 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "3b41f57ebc804f119281e8f47ff5ea72"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "z_gen.shape: torch.Size([9, 4, 32, 32])\n"
     ]
    }
   ],
   "source": [
    "plt.hist(z_gen.cpu().detach().numpy().flatten(), bins=100, log=True)\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# # VQ(z_gen)\n",
    "# h, w = z_gen.shape[2], z_gen.shape[3]\n",
    "# z_gen = rearrange(z_gen, 'b d h w -> b (h w) d')\n",
    "# z_gen, _ = trainer.pretrained_vq._codebook(z_gen)\n",
    "# z_gen = rearrange(z_gen, 'b (h w) d -> b d h w', h=h, w=w)\n",
    "\n",
    "# decode\n",
    "z_gen = rearrange(z_gen, 'b d h w -> b h w d')\n",
    "z_gen = trainer.pretrained_vq.project_out(z_gen)\n",
    "z_gen = rearrange(z_gen, 'b h w d -> b d h w')\n",
    "print('z_gen.shape:', z_gen.shape)\n",
    "\n",
    "x_gen = trainer.pretrained_decoder(z_gen)  # (b c h w)\n",
    "x_gen = x_gen.cpu().detach()\n",
    "x_gen = x_gen.argmax(dim=1)[:,None,:,:].float()\n",
    "print('x_gen.shape:', x_gen.shape)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "z_gen.shape: torch.Size([9, 64, 32, 32])\n",
      "x_gen.shape: torch.Size([9, 1, 128, 128])\n"
     ]
    }
   ],
   "source": [
    "# plot\n",
    "n_samples = x_gen.shape[0]\n",
    "n_rows = int(np.ceil(np.sqrt(n_samples)))\n",
    "fig, axes = plt.subplots(n_rows, n_rows, figsize=(12, 12))\n",
    "axes = axes.flatten()\n",
    "\n",
    "data = x_gen.numpy()  # (b 1 h w)\n",
    "data = np.flip(data, axis=2)  # (b 1 h w)\n",
    "data = data.squeeze()  # (b h w)\n",
    "for i in range(n_samples):\n",
    "    d = data[i]  # (h w)\n",
    "    axes[i].imshow(d)\n",
    "    axes[i].set_xticks([])\n",
    "    axes[i].set_yticks([])\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_cond.shape: torch.Size([9, 4, 128, 128])\n",
      "x_gen.shape: torch.Size([9, 1, 128, 128])\n"
     ]
    }
   ],
   "source": [
    "# sample many and save\n",
    "\n",
    "# sample z\n",
    "trainer.ema.ema_model.eval()\n",
    "z_gen = trainer.ema.ema_model.sample(batch_size=200)\n",
    "\n",
    "# sample x_gen\n",
    "z_gen = rearrange(z_gen, 'b d h w -> b h w d')\n",
    "z_gen = trainer.pretrained_vq.project_out(z_gen)\n",
    "z_gen = rearrange(z_gen, 'b h w d -> b d h w')\n",
    "print('z_gen.shape:', z_gen.shape)\n",
    "\n",
    "x_gen = trainer.pretrained_decoder(z_gen)  # (b c h w)\n",
    "x_gen = x_gen.cpu().detach()\n",
    "x_gen = x_gen.argmax(dim=1)[:,None,:,:].float()\n",
    "print('x_gen.shape:', x_gen.shape)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# save\n",
    "with open('X_gen_20230307.npy', 'wb') as f:\n",
    "    np.save(f, x_gen[:, 0, : ,:])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}