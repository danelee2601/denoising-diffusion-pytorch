dataset:
#  dataset_name: 'CBF'
  dirname: 'dataset/facies'
  img_size: 128
  train_ratio: 0.7
  in_channels: 4
  data_scaling: True
  batch_sizes:
    stage1: 256
    stage2: 128
  num_workers: 0

exp_params:
  LR: 0.001
  weight_decay: 0.00001

trainer_params:
  gpus:
    - 0
  max_epochs:
    stage1: 2000
    stage2: 10000


encoder:
  dim: 64
  n_resnet_blocks: 4
  downsampling_rate: 4  # the actual compression rate is `downsampling_rate ** 2` given both height and width.

decoder:
  dim: 64
  n_resnet_blocks: 4


VQ-VAE:  # hyper-parameter choice is made based on the LDM paper
  codebook_size: 256
  decay: 0.8
  commitment_weight: 1.
  codebook_dim: 4
  emb_dropout: 0.
  kmeans_init: False
  threshold_ema_dead_code: 0

diffusion:
  unet:
    self_condition: False


#MaskGIT:
#  choice_temperatures: # for masking
#    lf: 4
#    hf: 4
#  stochastic_sampling: 1  # from (Lee et al., 2022)
#  T: 10
#  prior_model:
#    hidden_dim: 256
#    n_layers: 4
#    heads: 2
#    ff_mult: 1
#    use_rmsnorm: True
#    p_unconditional: 0.2
#
#class_guidance:
#  guidance_scale: 1.